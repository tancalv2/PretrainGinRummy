{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RubzqOlEW1H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0defec8d-9067-4df7-dab9-0dccfb5f4bc7"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8lnqVu4Qq-8"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HnoQ7YHQ7eK"
      },
      "source": [
        "!pip install rlcard\n",
        "!pip install rlcard[torch]\n",
        "!pip install rlcard[tensorflow]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2wz3Kx41J9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd1846c-e240-48e9-a8cf-215ef88d38d4"
      },
      "source": [
        "# path\r\n",
        "pth = '/content/drive/MyDrive/Colab Notebooks/Thesis'\r\n",
        "\r\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Thesis/SupervisedLearning\r\n",
        "\r\n",
        "from models import *\r\n",
        "\r\n",
        "# %cd /content/drive/My Drive/Colab Notebooks/Thesis/dqn\r\n",
        "\r\n",
        "# import DQNAgent_pytorch as DQNAgent\r\n",
        "\r\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Thesis"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Thesis/SupervisedLearning\n",
            "/content/drive/My Drive/Colab Notebooks/Thesis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJX46H9x33XX"
      },
      "source": [
        "## DQN Customized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRvqQZTgy8U-"
      },
      "source": [
        "''' DQN agent\n",
        "The code was adapted from RLCard from https://github.com/datamllab/rlcard/blob/master/rlcard/agents/dqn_agent_pytorch.py\n",
        "The code is derived from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py\n",
        "Copyright (c) 2019 Matthew Judell\n",
        "Copyright (c) 2019 DATA Lab at Texas A&M University\n",
        "Copyright (c) 2016 Denny Britz\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple\n",
        "from copy import deepcopy\n",
        "\n",
        "from rlcard.agents.dqn_agent import Memory\n",
        "from rlcard.utils.utils import remove_illegal\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "\n",
        "class DQNAgent(object):\n",
        "    '''\n",
        "    Approximate clone of rlcard.agents.dqn_agent.DQNAgent\n",
        "    that depends on PyTorch instead of Tensorflow\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 scope,\n",
        "                 replay_memory_size=20000,\n",
        "                 replay_memory_init_size=100,\n",
        "                 update_target_estimator_every=1000,\n",
        "                 discount_factor=0.99,\n",
        "                 epsilon_start=1.0,\n",
        "                 epsilon_end=0.1,\n",
        "                 epsilon_decay_steps=20000,\n",
        "                 batch_size=32,\n",
        "                 action_num=2,\n",
        "                 state_shape=None,\n",
        "                 train_every=1,\n",
        "                 mlp_layers=None,\n",
        "                 learning_rate=0.00005,\n",
        "                 device=None):\n",
        "\n",
        "        '''\n",
        "        Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
        "        Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
        "        Args:\n",
        "            scope (str): The name of the DQN agent\n",
        "            replay_memory_size (int): Size of the replay memory\n",
        "            replay_memory_init_size (int): Number of random experiences to sampel when initializing\n",
        "              the reply memory.\n",
        "            update_target_estimator_every (int): Copy parameters from the Q estimator to the\n",
        "              target estimator every N steps\n",
        "            discount_factor (float): Gamma discount factor\n",
        "            epsilon_start (int): Chance to sample a random action when taking an action.\n",
        "              Epsilon is decayed over time and this is the start value\n",
        "            epsilon_end (int): The final minimum value of epsilon after decaying is done\n",
        "            epsilon_decay_steps (int): Number of steps to decay epsilon over\n",
        "            batch_size (int): Size of batches to sample from the replay memory\n",
        "            evaluate_every (int): Evaluate every N steps\n",
        "            action_num (int): The number of the actions\n",
        "            state_space (list): The space of the state vector\n",
        "            train_every (int): Train the network every X steps.\n",
        "            mlp_layers (list): The layer number and the dimension of each layer in MLP\n",
        "            learning_rate (float): The learning rate of the DQN agent.\n",
        "            device (torch.device): whether to use the cpu or gpu\n",
        "        '''\n",
        "        self.use_raw = False\n",
        "        self.scope = scope\n",
        "        self.replay_memory_init_size = replay_memory_init_size\n",
        "        self.update_target_estimator_every = update_target_estimator_every\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.action_num = action_num\n",
        "        self.train_every = train_every\n",
        "\n",
        "        # Torch device\n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        # Total timesteps\n",
        "        self.total_t = 0\n",
        "\n",
        "        # Total training step\n",
        "        self.train_t = 0\n",
        "\n",
        "        # The epsilon decay scheduler\n",
        "        self.epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
        "\n",
        "        # Create estimators\n",
        "        self.q_estimator = Estimator(action_num=action_num, learning_rate=learning_rate, state_shape=state_shape, \\\n",
        "            mlp_layers=mlp_layers, device=self.device)\n",
        "        self.target_estimator = Estimator(action_num=action_num, learning_rate=learning_rate, state_shape=state_shape, \\\n",
        "            mlp_layers=mlp_layers, device=self.device)\n",
        "\n",
        "        # Create replay memory\n",
        "        self.memory = Memory(replay_memory_size, batch_size)\n",
        "\n",
        "    def feed(self, ts):\n",
        "        ''' Store data in to replay buffer and train the agent. There are two stages.\n",
        "            In stage 1, populate the memory without training\n",
        "            In stage 2, train the agent every several timesteps\n",
        "        Args:\n",
        "            ts (list): a list of 5 elements that represent the transition\n",
        "        '''\n",
        "        (state, action, reward, next_state, done) = tuple(ts)\n",
        "        self.feed_memory(state['obs'], action, reward, next_state['obs'], done)\n",
        "        self.total_t += 1\n",
        "        tmp = self.total_t - self.replay_memory_init_size\n",
        "        if tmp>=0 and tmp%self.train_every == 0:\n",
        "            self.train()\n",
        "\n",
        "    def step(self, state):\n",
        "        ''' Predict the action for genrating training data but\n",
        "            have the predictions disconnected from the computation graph\n",
        "        Args:\n",
        "            state (numpy.array): current state\n",
        "        Returns:\n",
        "            action (int): an action id\n",
        "        '''\n",
        "        A = self.predict(state['obs'])\n",
        "        A = remove_illegal(A, state['legal_actions'])\n",
        "        action = np.random.choice(np.arange(len(A)), p=A)\n",
        "        return action\n",
        "\n",
        "    def eval_step(self, state):\n",
        "        ''' Predict the action for evaluation purpose.\n",
        "        Args:\n",
        "            state (numpy.array): current state\n",
        "        Returns:\n",
        "            action (int): an action id\n",
        "        '''\n",
        "        q_values = self.q_estimator.predict_nograd(np.expand_dims(state['obs'], 0))[0]\n",
        "        probs = remove_illegal(np.exp(q_values), state['legal_actions'])\n",
        "        best_action = np.argmax(probs)\n",
        "        return best_action, probs\n",
        "\n",
        "    def predict(self, state):\n",
        "        ''' Predict the action probabilities but have them\n",
        "            disconnected from the computation graph\n",
        "        Args:\n",
        "            state (numpy.array): current state\n",
        "        Returns:\n",
        "            q_values (numpy.array): a 1-d array where each entry represents a Q value\n",
        "        '''\n",
        "        epsilon = self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)]\n",
        "        A = np.ones(self.action_num, dtype=float) * epsilon / self.action_num\n",
        "        q_values = self.q_estimator.predict_nograd(np.expand_dims(state, 0))[0]\n",
        "        best_action = np.argmax(q_values)\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "\n",
        "    def train(self):\n",
        "        ''' Train the network\n",
        "        Returns:\n",
        "            loss (float): The loss of the current batch.\n",
        "        '''\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample()\n",
        "\n",
        "        # Calculate best next actions using Q-network (Double DQN)\n",
        "        q_values_next = self.q_estimator.predict_nograd(next_state_batch)\n",
        "        best_actions = np.argmax(q_values_next, axis=1)\n",
        "\n",
        "        # Evaluate best next actions using Target-network (Double DQN)\n",
        "        q_values_next_target = self.target_estimator.predict_nograd(next_state_batch)\n",
        "        target_batch = reward_batch + np.invert(done_batch).astype(np.float32) * \\\n",
        "            self.discount_factor * q_values_next_target[np.arange(self.batch_size), best_actions]\n",
        "\n",
        "        # Perform gradient descent update\n",
        "        state_batch = np.array(state_batch)\n",
        "\n",
        "        loss = self.q_estimator.update(state_batch, action_batch, target_batch)\n",
        "        print('\\rINFO - Agent {}, step {}, rl-loss: {}'.format(self.scope, self.total_t, loss), end='')\n",
        "\n",
        "        # Update the target estimator\n",
        "        if self.train_t % self.update_target_estimator_every == 0:\n",
        "            self.target_estimator = deepcopy(self.q_estimator)\n",
        "            print(\"\\nINFO - Copied model parameters to target network.\")\n",
        "\n",
        "        self.train_t += 1\n",
        "\n",
        "    def feed_memory(self, state, action, reward, next_state, done):\n",
        "        ''' Feed transition to memory\n",
        "        Args:\n",
        "            state (numpy.array): the current state\n",
        "            action (int): the performed action ID\n",
        "            reward (float): the reward received\n",
        "            next_state (numpy.array): the next state after performing the action\n",
        "            done (boolean): whether the episode is finished\n",
        "        '''\n",
        "        self.memory.save(state, action, reward, next_state, done)\n",
        "\n",
        "    def get_state_dict(self):\n",
        "        ''' Get the state dict to save models\n",
        "        Returns:\n",
        "            (dict): A dict of model states\n",
        "        '''\n",
        "        q_key = self.scope + '_q_estimator'\n",
        "        q_value = self.q_estimator.qnet.state_dict()\n",
        "        target_key = self.scope + '_target_estimator'\n",
        "        target_value = self.target_estimator.qnet.state_dict()\n",
        "        return {q_key: q_value, target_key: target_value}\n",
        "\n",
        "    def load(self, checkpoint):\n",
        "        ''' Load model\n",
        "        Args:\n",
        "            checkpoint (dict): the loaded state\n",
        "        '''\n",
        "        q_key = self.scope + '_q_estimator'\n",
        "        self.q_estimator.qnet.load_state_dict(checkpoint[q_key])\n",
        "        target_key = self.scope + '_target_estimator'\n",
        "        self.target_estimator.qnet.load_state_dict(checkpoint[target_key])\n",
        "\n",
        "class Estimator(object):\n",
        "    '''\n",
        "    Approximate clone of rlcard.agents.dqn_agent.Estimator that\n",
        "    uses PyTorch instead of Tensorflow.  All methods input/output np.ndarray.\n",
        "    Q-Value Estimator neural network.\n",
        "    This network is used for both the Q-Network and the Target Network.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, action_num=2, learning_rate=0.001, state_shape=None, mlp_layers=None, device=None):\n",
        "        ''' Initilalize an Estimator object.\n",
        "        Args:\n",
        "            action_num (int): the number output actions\n",
        "            state_shape (list): the shape of the state space\n",
        "            mlp_layers (list): size of outputs of mlp layers\n",
        "            device (torch.device): whether to use cpu or gpu\n",
        "        '''\n",
        "        self.action_num = action_num\n",
        "        self.learning_rate=learning_rate\n",
        "        self.state_shape = state_shape\n",
        "        self.mlp_layers = mlp_layers\n",
        "        self.device = device\n",
        "\n",
        "        # set up Q model and place it in eval mode\n",
        "        qnet = EstimatorNetwork(action_num, state_shape, mlp_layers)\n",
        "        qnet = qnet.to(self.device)\n",
        "        self.qnet = qnet\n",
        "        self.qnet.eval()\n",
        "\n",
        "        # initialize the weights using Xavier init\n",
        "        for p in self.qnet.parameters():\n",
        "            if len(p.data.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p.data)\n",
        "\n",
        "        # set up loss function\n",
        "        self.mse_loss = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # set up optimizer\n",
        "        self.optimizer =  torch.optim.Adam(self.qnet.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def predict_nograd(self, s):\n",
        "        ''' Predicts action values, but prediction is not included\n",
        "            in the computation graph.  It is used to predict optimal next\n",
        "            actions in the Double-DQN algorithm.\n",
        "        Args:\n",
        "          s (np.ndarray): (batch, state_len)\n",
        "        Returns:\n",
        "          np.ndarray of shape (batch_size, NUM_VALID_ACTIONS) containing the estimated\n",
        "          action values.\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            s = torch.from_numpy(s).float().to(self.device)\n",
        "            q_as = self.qnet(s).cpu().numpy()\n",
        "        return q_as\n",
        "\n",
        "    def update(self, s, a, y):\n",
        "        ''' Updates the estimator towards the given targets.\n",
        "            In this case y is the target-network estimated\n",
        "            value of the Q-network optimal actions, which\n",
        "            is labeled y in Algorithm 1 of Minh et al. (2015)\n",
        "        Args:\n",
        "          s (np.ndarray): (batch, state_shape) state representation\n",
        "          a (np.ndarray): (batch,) integer sampled actions\n",
        "          y (np.ndarray): (batch,) value of optimal actions according to Q-target\n",
        "        Returns:\n",
        "          The calculated loss on the batch.\n",
        "        '''\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        self.qnet.train()\n",
        "\n",
        "        s = torch.from_numpy(s).float().to(self.device)\n",
        "        a = torch.from_numpy(a).long().to(self.device)\n",
        "        y = torch.from_numpy(y).float().to(self.device)\n",
        "\n",
        "        # (batch, state_shape) -> (batch, action_num)\n",
        "        q_as = self.qnet(s)\n",
        "\n",
        "        # (batch, action_num) -> (batch, )\n",
        "        Q = torch.gather(q_as, dim=-1, index=a.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # update model\n",
        "        batch_loss = self.mse_loss(Q, y)\n",
        "        batch_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        batch_loss = batch_loss.item()\n",
        "\n",
        "        self.qnet.eval()\n",
        "\n",
        "        return batch_loss\n",
        "\n",
        "\n",
        "class EstimatorNetwork(nn.Module):\n",
        "    ''' The function approximation network for Estimator\n",
        "        It is just a series of sigmoid layers. All in/out are torch.tensor\n",
        "        (OLD) It is just a series of tanh layers. All in/out are torch.tensor\n",
        "    '''\n",
        "\n",
        "    def __init__(self, action_num=2, state_shape=None, mlp_layers=None):\n",
        "        ''' Initialize the Q network\n",
        "        Args:\n",
        "            action_num (int): number of legal actions\n",
        "            state_shape (list): shape of state tensor\n",
        "            mlp_layers (list): output size of each fc layer\n",
        "        '''\n",
        "        super(EstimatorNetwork, self).__init__()\n",
        "\n",
        "        self.action_num = action_num\n",
        "        self.state_shape = state_shape\n",
        "        self.mlp_layers = mlp_layers\n",
        "\n",
        "        # build the Q network\n",
        "        layer_dims = [np.prod(self.state_shape)] + self.mlp_layers\n",
        "        fc = [nn.Flatten()]\n",
        "        # fc.append(nn.BatchNorm1d(layer_dims[0]))\n",
        "        for i in range(len(layer_dims)-1):\n",
        "            fc.append(nn.Linear(layer_dims[i], layer_dims[i+1], bias=True))\n",
        "            fc.append(nn.Sigmoid())\n",
        "        fc.append(nn.Linear(layer_dims[-1], self.action_num, bias=True))\n",
        "        fc.append(nn.Softmax(dim=1))\n",
        "        self.fc_layers = nn.Sequential(*fc)\n",
        "\n",
        "    def forward(self, s):\n",
        "        ''' Predict action values\n",
        "        Args:\n",
        "            s  (Tensor): (batch, state_shape)\n",
        "        '''\n",
        "        return self.fc_layers(s)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSz3nMo_3_5W"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilBx3V-THxqe"
      },
      "source": [
        "from tensorflow.keras.backend import clear_session\n",
        "clear_session()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eeM3GH5Hxqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d834fd10-7ad9-41ca-ff2d-07995adc1ab8"
      },
      "source": [
        "import torch\n",
        "import tensorflow\n",
        "import os\n",
        "\n",
        "import rlcard\n",
        "# from rlcard.agents import DQNAgentPytorch as DQNAgent\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.utils import set_global_seed, tournament\n",
        "from rlcard.utils import Logger\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Make environment\n",
        "env = rlcard.make('gin-rummy', config={'seed': 0})\n",
        "eval_env = rlcard.make('gin-rummy', config={'seed': 0})\n",
        "env.game.settings.print_settings()\n",
        "\n",
        "# Set the iterations numbers and how frequently we evaluate/save plot\n",
        "evaluate_every = 100\n",
        "evaluate_num = 100  # mahjong_dqn has 1000\n",
        "episode_num = 5000  # mahjong_dqn has 100000\n",
        "# episode_num = 1000  # mahjong_dqn has 100000\n",
        "\n",
        "# The initial memory size\n",
        "memory_init_size = 1000\n",
        "\n",
        "# Train the agent every X steps\n",
        "train_every = 1\n",
        "\n",
        "# The paths for saving the logs and learning curves\n",
        "log_dir = './experiments/gin_rummy_dqn_result/all/all/all_states_all_actions_2hl_extra_knock_data_40K'\n",
        "\n",
        "# Set a global seed\n",
        "set_global_seed(0)\n",
        "\n",
        "agent = DQNAgent(scope='dqn',\n",
        "                 action_num=env.action_num,\n",
        "                 replay_memory_init_size=memory_init_size,\n",
        "                 train_every=train_every,\n",
        "                 state_shape=env.state_shape,\n",
        "                 mlp_layers=[520, 110],\n",
        "                 device=device)\n",
        "    # def __init__(self,\n",
        "    #              scope,\n",
        "    #              replay_memory_size=20000,\n",
        "    #              replay_memory_init_size=100,\n",
        "    #              update_target_estimator_every=1000,\n",
        "    #              discount_factor=0.99,\n",
        "    #              epsilon_start=1.0,\n",
        "    #              epsilon_end=0.1,\n",
        "    #              epsilon_decay_steps=20000,\n",
        "    #              batch_size=32,\n",
        "    #              action_num=2,\n",
        "    #              state_shape=None,\n",
        "    #              train_every=1,\n",
        "    #              mlp_layers=None,\n",
        "    #              learning_rate=0.00005,\n",
        "    #              device=None)\n",
        "\n",
        "state = 'all'\n",
        "action = 'all'\n",
        "model = torch.load('{}/models/{}/{}/all_states_all_actions_2hl_extra_knock_data_40K/model.pt'.format(pth,state,action), map_location=device)\n",
        "\n",
        "# load pretrained weights\n",
        "agent.q_estimator.qnet.fc_layers[1].weight = torch.nn.Parameter(model.l1.weight)\n",
        "agent.q_estimator.qnet.fc_layers[1].bias = torch.nn.Parameter(model.l1.bias)\n",
        "agent.q_estimator.qnet.fc_layers[3].weight = torch.nn.Parameter(model.l2.weight)\n",
        "agent.q_estimator.qnet.fc_layers[3].bias = torch.nn.Parameter(model.l2.bias)\n",
        "agent.q_estimator.qnet.fc_layers[5].weight = torch.nn.Parameter(model.l3.weight)\n",
        "agent.q_estimator.qnet.fc_layers[5].bias = torch.nn.Parameter(model.l3.bias)\n",
        "\n",
        "agent.target_estimator.qnet.fc_layers[1].weight = torch.nn.Parameter(model.l1.weight)\n",
        "agent.target_estimator.qnet.fc_layers[1].bias = torch.nn.Parameter(model.l1.bias)\n",
        "agent.target_estimator.qnet.fc_layers[3].weight = torch.nn.Parameter(model.l2.weight)\n",
        "agent.target_estimator.qnet.fc_layers[3].bias = torch.nn.Parameter(model.l2.bias)\n",
        "agent.target_estimator.qnet.fc_layers[5].weight = torch.nn.Parameter(model.l2.weight)\n",
        "agent.target_estimator.qnet.fc_layers[5].bias = torch.nn.Parameter(model.l2.bias)\n",
        "# agent.q_estimator.qnet.fc_layers[2].weight = torch.nn.Parameter(model.l1.weight)\n",
        "# agent.q_estimator.qnet.fc_layers[2].bias = torch.nn.Parameter(model.l1.bias)\n",
        "# agent.q_estimator.qnet.fc_layers[4].weight = torch.nn.Parameter(model.l2.weight)\n",
        "# agent.q_estimator.qnet.fc_layers[4].bias = torch.nn.Parameter(model.l2.bias)\n",
        "# agent.q_estimator.qnet.fc_layers[6].weight = torch.nn.Parameter(model.l3.weight)\n",
        "# agent.q_estimator.qnet.fc_layers[6].bias = torch.nn.Parameter(model.l3.bias)\n",
        "\n",
        "# agent.target_estimator.qnet.fc_layers[2].weight = torch.nn.Parameter(model.l1.weight)\n",
        "# agent.target_estimator.qnet.fc_layers[2].bias = torch.nn.Parameter(model.l1.bias)\n",
        "# agent.target_estimator.qnet.fc_layers[4].weight = torch.nn.Parameter(model.l2.weight)\n",
        "# agent.target_estimator.qnet.fc_layers[4].bias = torch.nn.Parameter(model.l2.bias)\n",
        "# agent.target_estimator.qnet.fc_layers[6].weight = torch.nn.Parameter(model.l2.weight)\n",
        "# agent.target_estimator.qnet.fc_layers[6].bias = torch.nn.Parameter(model.l2.bias)\n",
        "\n",
        "random_agent = RandomAgent(action_num=eval_env.action_num)\n",
        "env.set_agents([agent, random_agent])\n",
        "eval_env.set_agents([agent, random_agent])\n",
        "\n",
        "# Init a Logger to plot the learning curve\n",
        "logger = Logger(log_dir)\n",
        "\n",
        "for episode in range(episode_num):\n",
        "\n",
        "    # Generate data from the environment\n",
        "    trajectories, _ = env.run(is_training=True)\n",
        "\n",
        "    # Feed transitions into agent memory, and train the agent\n",
        "    for ts in trajectories[0]:\n",
        "        agent.feed(ts)\n",
        "\n",
        "    # Evaluate the performance. Play with random agents.\n",
        "    if episode % evaluate_every == 0:\n",
        "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
        "\n",
        "# Close files in the logger\n",
        "logger.close_files()\n",
        "\n",
        "# Plot the learning curve\n",
        "logger.plot('DQN')\n",
        "\n",
        "# Save model\n",
        "save_dir = 'models/gin_rummy_dqn_pytorch/all/all/all_states_all_actions_2hl_extra_knock_data_40K'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "state_dict = agent.get_state_dict()\n",
        "print(state_dict.keys())\n",
        "torch.save(state_dict, os.path.join(save_dir, 'model.pth'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== Settings ==========\n",
            "scorer_name=GinRummyScorer\n",
            "dealer_for_round=DealerForRound.Random\n",
            "stockpile_dead_card_count=2\n",
            "going_out_deadwood_count=10\n",
            "max_drawn_card_count=52\n",
            "is_allowed_knock=True\n",
            "is_allowed_gin=True\n",
            "is_allowed_pick_up_discard=True\n",
            "is_allowed_to_discard_picked_up_card=False\n",
            "is_always_knock=False\n",
            "is_south_never_knocks=False\n",
            "==============================\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  136\n",
            "  reward       |  0.20679999999999968\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 1000, rl-loss: 0.34822678565979004\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 2000, rl-loss: 0.5819202661514282\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 3000, rl-loss: 0.5173667073249817\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 4000, rl-loss: 0.37212714552879333\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 4484, rl-loss: 0.4371311664581299\n",
            "----------------------------------------\n",
            "  timestep     |  8940\n",
            "  reward       |  0.25059999999999966\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 5000, rl-loss: 0.3787386417388916\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 6000, rl-loss: 0.34643125534057617\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 7000, rl-loss: 0.34405744075775146\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 7815, rl-loss: 0.24240797758102417\n",
            "----------------------------------------\n",
            "  timestep     |  15536\n",
            "  reward       |  0.22149999999999967\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 8000, rl-loss: 0.39680275321006775\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 9000, rl-loss: 0.24672526121139526\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 10000, rl-loss: 0.3107530474662781\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 11000, rl-loss: 0.2942597568035126\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 11073, rl-loss: 0.22973871231079102\n",
            "----------------------------------------\n",
            "  timestep     |  21981\n",
            "  reward       |  0.2339999999999996\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 12000, rl-loss: 0.1614050716161728\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 13000, rl-loss: 0.32139748334884644\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 14000, rl-loss: 0.22623610496520996\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 14227, rl-loss: 0.28052419424057007\n",
            "----------------------------------------\n",
            "  timestep     |  28217\n",
            "  reward       |  0.2332999999999997\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 15000, rl-loss: 0.36124861240386963\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 16000, rl-loss: 0.19189992547035217\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 17000, rl-loss: 0.36281147599220276\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 17097, rl-loss: 0.19071972370147705\n",
            "----------------------------------------\n",
            "  timestep     |  33875\n",
            "  reward       |  0.2364999999999997\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 18000, rl-loss: 0.16573843359947205\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 19000, rl-loss: 0.17270514369010925\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 19851, rl-loss: 0.23332978785037994\n",
            "----------------------------------------\n",
            "  timestep     |  39304\n",
            "  reward       |  0.23779999999999962\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 20000, rl-loss: 0.3105917274951935\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 21000, rl-loss: 0.23502890765666962\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 22000, rl-loss: 0.22668306529521942\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 22615, rl-loss: 0.24398283660411835\n",
            "----------------------------------------\n",
            "  timestep     |  44751\n",
            "  reward       |  0.22189999999999965\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 23000, rl-loss: 0.21761450171470642\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 24000, rl-loss: 0.25523841381073\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 25000, rl-loss: 0.10580717027187347\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 25090, rl-loss: 0.10779968649148941\n",
            "----------------------------------------\n",
            "  timestep     |  49610\n",
            "  reward       |  0.2068999999999997\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 26000, rl-loss: 0.19696541130542755\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 27000, rl-loss: 0.137812077999115\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 27640, rl-loss: 0.22414995729923248\n",
            "----------------------------------------\n",
            "  timestep     |  54630\n",
            "  reward       |  0.23389999999999966\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 28000, rl-loss: 0.19737550616264343\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 29000, rl-loss: 0.10665322095155716\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 30000, rl-loss: 0.13767576217651367\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 30109, rl-loss: 0.20851722359657288\n",
            "----------------------------------------\n",
            "  timestep     |  59479\n",
            "  reward       |  0.2205999999999997\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 31000, rl-loss: 0.03661645948886871\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 32000, rl-loss: 0.17388443648815155\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 32594, rl-loss: 0.15599575638771057\n",
            "----------------------------------------\n",
            "  timestep     |  64360\n",
            "  reward       |  0.21419999999999956\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 33000, rl-loss: 0.1422167718410492\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 34000, rl-loss: 0.13037627935409546\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 35000, rl-loss: 0.11415168642997742\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 35324, rl-loss: 0.149134561419487\n",
            "----------------------------------------\n",
            "  timestep     |  69733\n",
            "  reward       |  0.19099999999999967\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 36000, rl-loss: 0.2405185103416443\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 37000, rl-loss: 0.0791400671005249\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 38000, rl-loss: 0.19850760698318481\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 38041, rl-loss: 0.20918968319892883\n",
            "----------------------------------------\n",
            "  timestep     |  75084\n",
            "  reward       |  0.23229999999999962\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 39000, rl-loss: 0.0923638641834259\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 40000, rl-loss: 0.10635954886674881\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 40734, rl-loss: 0.09063182771205902\n",
            "----------------------------------------\n",
            "  timestep     |  80384\n",
            "  reward       |  0.19389999999999968\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 41000, rl-loss: 0.1358259618282318\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 42000, rl-loss: 0.05844239890575409\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 43000, rl-loss: 0.11972387135028839\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 43360, rl-loss: 0.14808309078216553\n",
            "----------------------------------------\n",
            "  timestep     |  85548\n",
            "  reward       |  0.23759999999999967\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 44000, rl-loss: 0.1221623495221138\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 45000, rl-loss: 0.0866028219461441\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 45875, rl-loss: 0.15868020057678223\n",
            "----------------------------------------\n",
            "  timestep     |  90492\n",
            "  reward       |  0.1892999999999997\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 46000, rl-loss: 0.11024434864521027\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 47000, rl-loss: 0.0903211459517479\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 48000, rl-loss: 0.06699444353580475\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 48484, rl-loss: 0.07509782910346985\n",
            "----------------------------------------\n",
            "  timestep     |  95622\n",
            "  reward       |  0.20009999999999967\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 49000, rl-loss: 0.12379935383796692\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 50000, rl-loss: 0.198449969291687\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 51000, rl-loss: 0.13698765635490417\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 51052, rl-loss: 0.08468592911958694\n",
            "----------------------------------------\n",
            "  timestep     |  100672\n",
            "  reward       |  0.22609999999999966\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 52000, rl-loss: 0.1463213860988617\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 53000, rl-loss: 0.1303069293498993\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 53576, rl-loss: 0.1164587214589119\n",
            "----------------------------------------\n",
            "  timestep     |  105629\n",
            "  reward       |  0.23789999999999964\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 54000, rl-loss: 0.11058909446001053\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 55000, rl-loss: 0.15781480073928833\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 56000, rl-loss: 0.11711452901363373\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 56320, rl-loss: 0.06056325137615204\n",
            "----------------------------------------\n",
            "  timestep     |  111036\n",
            "  reward       |  0.21909999999999966\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 57000, rl-loss: 0.1610599160194397\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 58000, rl-loss: 0.11523036658763885\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 58804, rl-loss: 0.1726073920726776\n",
            "----------------------------------------\n",
            "  timestep     |  115913\n",
            "  reward       |  0.22779999999999961\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 59000, rl-loss: 0.18385140597820282\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 60000, rl-loss: 0.1275351345539093\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 61000, rl-loss: 0.1793360710144043\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 61687, rl-loss: 0.1091003268957138\n",
            "----------------------------------------\n",
            "  timestep     |  121595\n",
            "  reward       |  0.23489999999999964\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 62000, rl-loss: 0.18938499689102173\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 63000, rl-loss: 0.09066161513328552\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 64000, rl-loss: 0.15209311246871948\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 64263, rl-loss: 0.1803693026304245\n",
            "----------------------------------------\n",
            "  timestep     |  126662\n",
            "  reward       |  0.20799999999999963\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 65000, rl-loss: 0.16495326161384583\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 66000, rl-loss: 0.1088123768568039\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 66927, rl-loss: 0.06245271861553192\n",
            "----------------------------------------\n",
            "  timestep     |  131900\n",
            "  reward       |  0.2438999999999996\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 67000, rl-loss: 0.14219129085540771\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 68000, rl-loss: 0.09559258818626404\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 69000, rl-loss: 0.10011854022741318\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 69523, rl-loss: 0.172125905752182\n",
            "----------------------------------------\n",
            "  timestep     |  137011\n",
            "  reward       |  0.21809999999999963\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 70000, rl-loss: 0.1971028745174408\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 71000, rl-loss: 0.10065625607967377\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 71980, rl-loss: 0.1059102714061737\n",
            "----------------------------------------\n",
            "  timestep     |  141839\n",
            "  reward       |  0.21959999999999963\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 72000, rl-loss: 0.10267584025859833\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 73000, rl-loss: 0.08450760692358017\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 74000, rl-loss: 0.09433729946613312\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 74628, rl-loss: 0.22804975509643555\n",
            "----------------------------------------\n",
            "  timestep     |  147052\n",
            "  reward       |  0.23109999999999956\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 75000, rl-loss: 0.1784452646970749\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 76000, rl-loss: 0.0803820863366127\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 77000, rl-loss: 0.16368433833122253\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 77255, rl-loss: 0.199933260679245\n",
            "----------------------------------------\n",
            "  timestep     |  152220\n",
            "  reward       |  0.22019999999999962\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 78000, rl-loss: 0.12905240058898926\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 79000, rl-loss: 0.22816841304302216\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 80000, rl-loss: 0.0811178907752037\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 80023, rl-loss: 0.12600421905517578\n",
            "----------------------------------------\n",
            "  timestep     |  157669\n",
            "  reward       |  0.23239999999999966\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 81000, rl-loss: 0.12678629159927368\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 82000, rl-loss: 0.09937821328639984\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 82951, rl-loss: 0.16904038190841675\n",
            "----------------------------------------\n",
            "  timestep     |  163445\n",
            "  reward       |  0.22409999999999963\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 83000, rl-loss: 0.06879008561372757\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 84000, rl-loss: 0.14665444195270538\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 85000, rl-loss: 0.16056522727012634\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 85696, rl-loss: 0.1343085616827011\n",
            "----------------------------------------\n",
            "  timestep     |  168852\n",
            "  reward       |  0.24799999999999955\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 86000, rl-loss: 0.15704216063022614\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 87000, rl-loss: 0.16067469120025635\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 88000, rl-loss: 0.24769780039787292\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 88295, rl-loss: 0.14354607462882996\n",
            "----------------------------------------\n",
            "  timestep     |  173962\n",
            "  reward       |  0.18019999999999972\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 89000, rl-loss: 0.06930333375930786\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 90000, rl-loss: 0.15326590836048126\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 90961, rl-loss: 0.10275981575250626\n",
            "----------------------------------------\n",
            "  timestep     |  179209\n",
            "  reward       |  0.22499999999999962\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 91000, rl-loss: 0.1482362151145935\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 92000, rl-loss: 0.12727998197078705\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 93000, rl-loss: 0.059969376772642136\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 93673, rl-loss: 0.1842857003211975\n",
            "----------------------------------------\n",
            "  timestep     |  184549\n",
            "  reward       |  0.19079999999999964\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 94000, rl-loss: 0.12509208917617798\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 95000, rl-loss: 0.03721077740192413\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 96000, rl-loss: 0.07711237668991089\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 96392, rl-loss: 0.1572614312171936\n",
            "----------------------------------------\n",
            "  timestep     |  189908\n",
            "  reward       |  0.22589999999999968\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 97000, rl-loss: 0.15153130888938904\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 98000, rl-loss: 0.08677826821804047\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 99000, rl-loss: 0.16937093436717987\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 99190, rl-loss: 0.1123097762465477\n",
            "----------------------------------------\n",
            "  timestep     |  195423\n",
            "  reward       |  0.2078999999999996\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 100000, rl-loss: 0.12931428849697113\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 101000, rl-loss: 0.29227402806282043\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 102000, rl-loss: 0.1417844444513321\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 102030, rl-loss: 0.14215561747550964\n",
            "----------------------------------------\n",
            "  timestep     |  201024\n",
            "  reward       |  0.19649999999999967\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 103000, rl-loss: 0.13700367510318756\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 104000, rl-loss: 0.14250725507736206\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 104874, rl-loss: 0.11147604882717133\n",
            "----------------------------------------\n",
            "  timestep     |  206627\n",
            "  reward       |  0.20289999999999964\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 105000, rl-loss: 0.2456706166267395\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 106000, rl-loss: 0.2061411291360855\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 107000, rl-loss: 0.14961016178131104\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 107367, rl-loss: 0.11250495165586472\n",
            "----------------------------------------\n",
            "  timestep     |  211527\n",
            "  reward       |  0.24079999999999965\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 108000, rl-loss: 0.12542203068733215\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 109000, rl-loss: 0.0584123320877552\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 110000, rl-loss: 0.025715993717312813\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 110161, rl-loss: 0.07573134452104568\n",
            "----------------------------------------\n",
            "  timestep     |  217033\n",
            "  reward       |  0.24259999999999965\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 111000, rl-loss: 0.1766861379146576\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 112000, rl-loss: 0.04712585359811783\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 112832, rl-loss: 0.1618550568819046\n",
            "----------------------------------------\n",
            "  timestep     |  222292\n",
            "  reward       |  0.23399999999999962\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 113000, rl-loss: 0.19108861684799194\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 114000, rl-loss: 0.1580870896577835\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 115000, rl-loss: 0.1214781254529953\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 115498, rl-loss: 0.16000279784202576\n",
            "----------------------------------------\n",
            "  timestep     |  227537\n",
            "  reward       |  0.21379999999999963\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 116000, rl-loss: 0.11977078020572662\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 117000, rl-loss: 0.051210738718509674\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 118000, rl-loss: 0.10365815460681915\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 118234, rl-loss: 0.11988392472267151\n",
            "----------------------------------------\n",
            "  timestep     |  232928\n",
            "  reward       |  0.22059999999999957\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 119000, rl-loss: 0.23235295712947845\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 120000, rl-loss: 0.06651918590068817\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 121000, rl-loss: 0.2209266722202301\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 121020, rl-loss: 0.10965671390295029\n",
            "----------------------------------------\n",
            "  timestep     |  238416\n",
            "  reward       |  0.2387999999999997\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 122000, rl-loss: 0.10931562632322311\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 123000, rl-loss: 0.09917954355478287\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 123708, rl-loss: 0.22566534578800201\n",
            "----------------------------------------\n",
            "  timestep     |  243713\n",
            "  reward       |  0.20589999999999972\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 124000, rl-loss: 0.07791734486818314\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 125000, rl-loss: 0.224219411611557\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 126000, rl-loss: 0.1204269677400589\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 126414, rl-loss: 0.1278558075428009\n",
            "----------------------------------------\n",
            "  timestep     |  249049\n",
            "  reward       |  0.22489999999999966\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 127000, rl-loss: 0.1526777446269989\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 128000, rl-loss: 0.18794766068458557\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 129000, rl-loss: 0.13171836733818054\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 129096, rl-loss: 0.15334272384643555\n",
            "----------------------------------------\n",
            "  timestep     |  254326\n",
            "  reward       |  0.20979999999999963\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 130000, rl-loss: 0.09852379560470581\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 131000, rl-loss: 0.10538141429424286\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 131623, rl-loss: 0.11288915574550629\n",
            "----------------------------------------\n",
            "  timestep     |  259295\n",
            "  reward       |  0.23169999999999963\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 132000, rl-loss: 0.13861557841300964\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 133000, rl-loss: 0.1711367517709732\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 134000, rl-loss: 0.08378840982913971\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 134451, rl-loss: 0.12399077415466309\n",
            "----------------------------------------\n",
            "  timestep     |  264873\n",
            "  reward       |  0.2384999999999996\n",
            "----------------------------------------\n",
            "INFO - Agent dqn, step 135000, rl-loss: 0.1426946222782135\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 136000, rl-loss: 0.06702625751495361\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 137000, rl-loss: 0.06009569764137268\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Agent dqn, step 137152, rl-loss: 0.07995922863483429./experiments/gin_rummy_dqn_result/all/all/all_states_all_actions_2hl_extra_knock_data_40K/performance.csv\n",
            "dict_keys(['dqn_q_estimator', 'dqn_target_estimator'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}